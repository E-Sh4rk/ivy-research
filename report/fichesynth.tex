
\documentclass{article}

\usepackage[francais]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{a4wide}
\usepackage{palatino}

\let\bfseriesbis=\bfseries \def\bfseries{\sffamily\bfseriesbis}


\newenvironment{point}[1]%
{\subsection*{#1}}%
{}

\setlength{\parskip}{0.3\baselineskip}

\begin{document}
\pagenumbering{gobble}

\title{Generalization of counterexamples for inductive invariant synthesis}

\author{MickaÃ«l Laurent, supervised by Bryan Parno\\Carnegie Mellon University}

\date{March-July 2018}

\maketitle

\pagestyle{empty} %
\thispagestyle{empty}

%% Attention: pas plus d'un recto-verso!


\begin{point}{General context}
  
  As distributed programs become more and more widespread, their verification is a major challenge.
  The general purpose of my internship is to make the certification of those programs easier, by providing new methods and semi-automated tools that will assist the user in this process.
  In this report, we are using IVy, a language combined with a set of tools that can be used to write certified distributed protocols.
  The particularity of IVy is that it restricts the language and the logic used for the specification in order to ensure some decidability properties.
  In particular, checking whether an inductive invariant is correct or not become decidable.

  When writing a protocol using IVy, the two main challenges are
  \begin{itemize}
    \item To implement the protocol and specify it by staying in the decidable fragment imposed by IVy,
    \item Once the previous point is done, to find an inductive invariant.
  \end{itemize}

  During this internship, I have been working on the second point.

\end{point}

\begin{point}{The problem}
  
  The problem we want to solve is the following:
  given an invariant that is not inductive, we want to strenghten it by generating a new (correct) invariant as strong as possible,
  if possible without breaking the decidability of the system.
  This process can be repeated until an inductive invariant is found.

  The current approach to solve this problem is the following:
  \begin{enumerate}
    \item When the invariant is not inductive, a finite counterexample is generated (it can be done in a decidable way).
    \item From this counterexample, a new universal invariant (invariant of the form \(\forall^*\)) is generated. This new invariant aims to `generalize' the counterexample.
  \end{enumerate}
  
  During this internship, I have tried to improve the second point of this process.

\end{point}

\begin{point}{Contributions}

  I have proposed a new way to `generalize' the counterexample. While the current method is based on the
  minimization of a set of constraints using \textit{symbolic bounded verification},
  my approach is based on an analysis of the execution of the counterexample.
  More precisely, I compute a set of constraints that make the current invariant broken at the end of the counterexample,
  and I retropopagate this set of constraints through the execution of the counterexample.
  These constraints are then generalized and the resulting formula is negated.
  This approach also generates universal invariant. It is also quite complementary with the current method
  since both can be combined in order to improve the result.

  Moreover, this new approach can be extended with an additional process of `weakening' that
  allows the generation of invariants of the form \(\forall^*\exists^*\) when there is no
  correct universal invariant that can capture the counterexample (in this case, the current method would fail).
  An incorrect universal invariant \(I\) is first generated, and a state `witness' of this incorrectness is searched.
  This state is `generalized' in a similar way as it was done for the counterexample, and then \(I\)
  is weakened in order to allow this kind of state. This process is repeated until \(I\) become correct.

  I have implemented a proof of concept for this new approach, using the languages F\# and OCaml, and the SMT-solver Z3.
 
\end{point}

\begin{point}{Results and guarantees}

  This new approach can sometimes generate universal invariants that are weaker than those generated by the current
  method. However, the two approaches can be combined: a set of constraints can first be generated using my approach,
  and this set can then be minimized using the current method.

  While the current method will fail if no universal invariant exists,
  my approach can also generate \(\forall^*\exists^*\) invariants (however, there is no guarantee of termination in this case).

  The new invariants generated with my approach are always correct if the current invariants (those that we want to make inductive)
  are also correct. In compariosn, symbolic bounded verification can help us to detect incorrect invariants, but if the boundary used is too small,
  incorrect invariants can be generated.

  I have tested this new approach on some basic examples, using my proof of concept.
  It was able to successfully find an inductive invariant for an implementation of a queue,
  while the current method would have failed.

\end{point}


\begin{point}{Review and prospects}

  My contribution is an improvement in the domain of the invariant synthesis.
  However, it relies on some decidability properties like being able to find finite counterexamples (which sometimes does not even exist).
  It requires to have some restrictions concerning the implementation and the specifications.
  Thereby, it can be used for programs written using IVy, but not in the general case.
  For instance, it can't be used to find inductive invariants for F* programs (F* being another language that is used to write certified programs).
  
  A lot of recent papers describe some methods to implement and specify programs by staying in a decidable fragment of first order logic (see bibliography).
  However, these methods can't be fully automated and they sometimes complexify the code. Moreover, it is sometimes not possible
  to write very generic pieces of code, because it could break decidability.
  I think these issues could be very problematic when implementing and certifying large-scale programs.
  Maybe we should look for a compromise, where we could benefit from a large expressivity in some fragments of code, and local decidability in some other fragments?

\end{point}


\end{document}







